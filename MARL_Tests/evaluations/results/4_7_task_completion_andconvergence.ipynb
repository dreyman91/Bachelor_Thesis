{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### QMIX Training",
   "id": "85af8391211b133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T15:04:27.679444Z",
     "start_time": "2025-05-20T15:04:27.435012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from mpe2 import simple_spread_v3\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "from failure_api.communication_models import ProbabilisticModel\n",
    "from failure_api.wrappers import CommunicationWrapper\n",
    "\n",
    "def make_env(num_agents=3, use_failure_api=False, failure_prob=0.5, seed=42, max_cycles=25):\n",
    "    env = simple_spread_v3.env(N=num_agents, max_cycles=max_cycles)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    if use_failure_api:\n",
    "        model = ProbabilisticModel(agent_ids=env.possible_agents, failure_prob=failure_prob)\n",
    "        env = CommunicationWrapper(env, failure_models=[model])\n",
    "        \n",
    "        \n",
    "    return aec_to_parallel(env)\n"
   ],
   "id": "e3f4cc907e2a406e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "\n",
    "| Component       | Description                                                      |\n",
    "| --------------- | ---------------------------------------------------------------- |\n",
    "| `AgentQNetwork` | Shared network for all agents to estimate individual Q-values    |\n",
    "| `MixerNetwork`  | Combines all agent Qs into a global Q value conditioned on state |\n",
    "| `QMIXPolicy`    | Combines agents + mixer + action selection (ε-greedy)            |\n"
   ],
   "id": "53c2110dfff98a00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T15:04:30.531790Z",
     "start_time": "2025-05-20T15:04:27.680450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)  \n",
    "    \n",
    "class MixerNetwork(nn.Module):\n",
    "    def __init__(self, num_agents, state_dim, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        self.hyper_w1 = nn.Linear(state_dim, num_agents * embed_dim)\n",
    "        self.hyper_w2 = nn.Linear(state_dim, embed_dim)\n",
    "        self.hyper_b1 = nn.Linear(state_dim, embed_dim)\n",
    "        self.hyper_b2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, agent_qs, state):\n",
    "        \"\"\"\n",
    "        agent_qs: (batch, num_agents)\n",
    "        state: (batch, state_dim)\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        \n",
    "        w1 = self.hyper_w1(state).view(bs, self.num_agents, -1)\n",
    "        b1 = self.hyper_b1(state).view(bs, 1, -1)   \n",
    "        hidden = F.elu(torch.bmm(agent_qs.unsqueeze(1), w1) + b1)\n",
    "        \n",
    "        w2 = self.hyper_w2(state).view(bs, -1, 1)  \n",
    "        b2 = self.hyper_b2(state).view(bs, 1, 1) \n",
    "        \n",
    "        q_total = torch.bmm(hidden, w2) + b2\n",
    "        return q_total.squeeze(-1).squeeze(-1) "
   ],
   "id": "d90c5306229fc3b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## QMIX Policy\n",
    "\n",
    "Shared AgentQNetwork\n",
    "\n",
    "Global MixerNetwork\n",
    "\n",
    "Epsilon-greedy exploration\n",
    "\n",
    "Centralized training + decentralized execution"
   ],
   "id": "74e1d165f959a14b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T15:04:30.551534Z",
     "start_time": "2025-05-20T15:04:30.532796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class QMIXPolicy:\n",
    "    def __init__(self, obs_dim, act_dim, state_dim, num_agents, device=\"cpu\",\n",
    "                 gamma=0.99, lr=5e-4, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=5000):\n",
    "        self.num_agents = num_agents\n",
    "        self.act_dim = act_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Shared agent network\n",
    "        self.agent_net = AgentQNetwork(obs_dim, act_dim).to(device)\n",
    "        self.target_agent_net = AgentQNetwork(obs_dim, act_dim).to(device)\n",
    "        self.target_agent_net.load_state_dict(self.agent_net.state_dict())\n",
    "\n",
    "        # Mixer\n",
    "        self.mixer_net = MixerNetwork(num_agents, state_dim).to(device)\n",
    "        self.target_mixer_net = MixerNetwork(num_agents, state_dim).to(device)\n",
    "        self.target_mixer_net.load_state_dict(self.mixer_net.state_dict())\n",
    "        self.agent_q_net = AgentQNetwork(obs_dim, act_dim)  # example\n",
    "        self.mixing_net = MixerNetwork(num_agents, state_dim)  # example\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.agent_net.parameters()) + list(self.mixer_net.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"agent_q_net\": self.agent_q_net.state_dict(),\n",
    "            \"mixing_net\": self.mixing_net.state_dict(),\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.agent_q_net.load_state_dict(state_dict[\"agent_q_net\"])\n",
    "        self.mixing_net.load_state_dict(state_dict[\"mixing_net\"])\n",
    "\n",
    "    def select_actions(self, obs_batch, explore=True):\n",
    "        \"\"\"\n",
    "        obs_batch: Dict[str, np.array], each obs shape = (obs_dim,)\n",
    "        \"\"\"\n",
    "        self.total_steps += 1\n",
    "        epsilon = self._epsilon()\n",
    "\n",
    "        actions = {}\n",
    "        for agent_id, obs in obs_batch.items():\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "            q_values = self.agent_net(obs_tensor)\n",
    "            if explore and np.random.rand() < epsilon:\n",
    "                action = np.random.randint(self.act_dim)\n",
    "            else:\n",
    "                action = torch.argmax(q_values).item()\n",
    "            actions[agent_id] = action\n",
    "        return actions\n",
    "\n",
    "    def _epsilon(self):\n",
    "        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "               np.exp(-1.0 * self.total_steps / self.epsilon_decay)\n",
    "    \n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"\n",
    "        batch: dict with keys: obs, actions, rewards, next_obs, states, next_states, dones\n",
    "        Shapes:\n",
    "          obs:      (B, num_agents, obs_dim)\n",
    "          actions:  (B, num_agents)\n",
    "          rewards:  (B,)\n",
    "          next_obs: (B, num_agents, obs_dim)\n",
    "          states:   (B, state_dim)\n",
    "          dones:    (B,)\n",
    "        \"\"\"\n",
    "        obs = torch.tensor(batch[\"obs\"], dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(batch[\"actions\"], dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(batch[\"rewards\"], dtype=torch.float32).to(self.device)\n",
    "        next_obs = torch.tensor(batch[\"next_obs\"], dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(batch[\"dones\"], dtype=torch.float32).to(self.device)\n",
    "        states = torch.tensor(batch[\"states\"], dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(batch[\"next_states\"], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        B = obs.shape[0]\n",
    "        agent_qs = self.agent_net(obs.view(-1, obs.shape[-1]))  # (B*num_agents, act_dim)\n",
    "        agent_qs = agent_qs.view(B, self.num_agents, self.act_dim)\n",
    "        chosen_qs = torch.gather(agent_qs, dim=2, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Target Q\n",
    "        with torch.no_grad():\n",
    "            next_agent_qs = self.target_agent_net(next_obs.view(-1, next_obs.shape[-1]))\n",
    "            next_agent_qs = next_agent_qs.view(B, self.num_agents, self.act_dim)\n",
    "            max_next_qs = next_agent_qs.max(dim=2)[0]  # (B, num_agents)\n",
    "\n",
    "        # Mixer\n",
    "        q_total = self.mixer_net(chosen_qs, states)\n",
    "        next_q_total = self.target_mixer_net(max_next_qs, next_states)\n",
    "\n",
    "        targets = rewards + self.gamma * (1 - dones) * next_q_total\n",
    "\n",
    "        loss = nn.MSELoss()(q_total, targets.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target(self, tau=0.01):\n",
    "        for target_param, param in zip(self.target_agent_net.parameters(), self.agent_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "        for target_param, param in zip(self.target_mixer_net.parameters(), self.mixer_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ],
   "id": "6bca48ea8a144b82",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Replay Buffer",
   "id": "5888e2a4fdd435f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T15:04:30.561524Z",
     "start_time": "2025-05-20T15:04:30.552538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# replay_buffer.py\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, num_agents, obs_dim, state_dim):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.num_agents = num_agents\n",
    "        self.obs_dim = obs_dim\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "    def add(self, obs, actions, reward, next_obs, state, next_state, done):\n",
    "        \"\"\"\n",
    "        obs, next_obs: dict of obs arrays per agent\n",
    "        actions: dict of int\n",
    "        \"\"\"\n",
    "        obs_arr = np.array([obs[a] for a in sorted(obs)])\n",
    "        next_obs_arr = np.array([next_obs[a] for a in sorted(next_obs)])\n",
    "        action_arr = np.array([actions[a] for a in sorted(actions)])\n",
    "\n",
    "        transition = {\n",
    "            \"obs\": obs_arr,\n",
    "            \"actions\": action_arr,\n",
    "            \"rewards\": reward,\n",
    "            \"next_obs\": next_obs_arr,\n",
    "            \"states\": state,\n",
    "            \"next_states\": next_state,\n",
    "            \"dones\": done\n",
    "        }\n",
    "\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return {\n",
    "            \"obs\": np.stack([x[\"obs\"] for x in batch]),\n",
    "            \"actions\": np.stack([x[\"actions\"] for x in batch]),\n",
    "            \"rewards\": np.array([x[\"rewards\"] for x in batch]),\n",
    "            \"next_obs\": np.stack([x[\"next_obs\"] for x in batch]),\n",
    "            \"states\": np.stack([x[\"states\"] for x in batch]),\n",
    "            \"next_states\": np.stack([x[\"next_states\"] for x in batch]),\n",
    "            \"dones\": np.array([x[\"dones\"] for x in batch])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "6678eb415c4914d6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "cf8c1d6e842f020c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T15:04:30.575146Z",
     "start_time": "2025-05-20T15:04:30.563940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_state(obs_dict, agent_ids, obs_dim):\n",
    "    \"\"\"\n",
    "    Returns concatenated observation vector across all agents.\n",
    "    Missing agents are padded with zeros (if terminated).\n",
    "    \"\"\"\n",
    "    return np.concatenate([\n",
    "        obs_dict.get(a, np.zeros(obs_dim)) for a in sorted(agent_ids)\n",
    "    ])\n",
    "\n",
    "def train_qmix(use_failure_api=False, episodes=1000, batch_size=32, buffer_capacity=10000):\n",
    "    env = make_env(num_agents=3, use_failure_api=use_failure_api)\n",
    "    agent_ids = env.possible_agents\n",
    "    obs_sample = env.reset()[0]\n",
    "    obs_dim = obs_sample[agent_ids[0]].shape[0]\n",
    "    act_dim = env.action_space(agent_ids[0]).n\n",
    "    state_dim = obs_dim * len(agent_ids)\n",
    "\n",
    "    policy = QMIXPolicy(obs_dim, act_dim, state_dim, len(agent_ids))\n",
    "    buffer = ReplayBuffer(buffer_capacity, len(agent_ids), obs_dim, state_dim)\n",
    "\n",
    "    rewards_log = []\n",
    "\n",
    "    for ep in trange(episodes, desc=\"Training\"):\n",
    "        obs_dict, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            actions = policy.select_actions(obs_dict, explore=True)\n",
    "            next_obs_dict, reward_dict, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            reward = sum(reward_dict.values())\n",
    "            done = all(list(terminations.values()) + list(truncations.values()))\n",
    "            if all(a in obs_dict and a in next_obs_dict for a in agent_ids):\n",
    "                state = get_state(obs_dict, agent_ids, obs_dim)\n",
    "                next_state = get_state(next_obs_dict, agent_ids, obs_dim)\n",
    "            \n",
    "                buffer.add(obs_dict, actions, reward, next_obs_dict, state, next_state, done)\n",
    "                obs_dict = next_obs_dict\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "            if len(buffer) > batch_size:\n",
    "                batch = buffer.sample(batch_size)\n",
    "                loss = policy.update(batch)\n",
    "                policy.update_target()\n",
    "\n",
    "        rewards_log.append((ep, total_reward, steps))\n",
    "        for i, x in enumerate(buffer.buffer):\n",
    "            try:\n",
    "                np.stack([x[\"next_obs\"] for x in buffer.buffer])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Problem at index {i}: {e}\")\n",
    "                break\n",
    "\n",
    "        if ep % 100 == 0:\n",
    "            avg_reward = np.mean([r[1] for r in rewards_log[-100:]])\n",
    "            torch.save(policy.state_dict(), r\"C:\\Users\\koste\\venv\\Bachelor_Thesis\\QMIX_Training\\qmix_ep{ep}.pt\")\n",
    "            print(f\"[Episode {ep}] Avg Reward: {avg_reward:.2f} Steps: {steps}\")\n",
    "\n",
    "    return rewards_log\n",
    "\n"
   ],
   "id": "5ded017eefa9fe7c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T18:48:51.449362Z",
     "start_time": "2025-05-20T15:04:30.576688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_baseline = train_qmix(use_failure_api=False, episodes=500)\n",
    "log_failure = train_qmix(use_failure_api=True, episodes=500)\n"
   ],
   "id": "9e5b145b83ace2e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29b12e9c92c24910b992971880136550"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Avg Reward: -72.51 Steps: 25\n",
      "[Episode 100] Avg Reward: -100.20 Steps: 25\n",
      "[Episode 200] Avg Reward: -143.57 Steps: 25\n",
      "[Episode 300] Avg Reward: -174.81 Steps: 25\n",
      "[Episode 400] Avg Reward: -185.78 Steps: 25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m log_baseline = \u001B[43mtrain_qmix\u001B[49m\u001B[43m(\u001B[49m\u001B[43muse_failure_api\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m log_failure = train_qmix(use_failure_api=\u001B[38;5;28;01mTrue\u001B[39;00m, episodes=\u001B[32m500\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 52\u001B[39m, in \u001B[36mtrain_qmix\u001B[39m\u001B[34m(use_failure_api, episodes, batch_size, buffer_capacity)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(buffer.buffer):\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         np.stack(\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnext_obs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m     53\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     54\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m⚠️ Problem at index \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 52\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(buffer.buffer):\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         np.stack([x[\u001B[33m\"\u001B[39m\u001B[33mnext_obs\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m buffer.buffer])\n\u001B[32m     53\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     54\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m⚠️ Problem at index \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "71f58ed51e1c468",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
