{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance Overhead\n",
    "\n",
    "### We use Probabilistic model because :\n",
    "\n",
    "Simplicity:\n",
    "It‚Äôs computationally light and does not depend on dynamic inputs (like agent distance, signal strength, or topology). This ensures the performance overhead comes purely from the API's wrapper logic, not the model's complexity.\n",
    "\n",
    "Consistent Timing:\n",
    "It uses simple random number generation per agent pair. This avoids fluctuations caused by environment-specific variables, which would skew the timing and memory readings.\n",
    "\n",
    "Baseline Comparability:\n",
    "Since the only thing changing between baseline and wrapped version is the failure mask logic (not the observation structure or graph computation), the % overhead is meaningful and isolated."
   ],
   "id": "7bbac1cc2c3f155b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:49:08.383345Z",
     "start_time": "2025-05-21T07:49:07.880562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "from mpe2 import simple_spread_v3\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from failure_api.wrappers import CommunicationWrapper\n",
    "from failure_api.communication_models import ProbabilisticModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Disable multiprocessing warnings\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "if hasattr(mp, \"set_start_method\"):\n",
    "    try:\n",
    "        mp.set_start_method(\"spawn\", force=True)\n",
    "    except RuntimeError:\n",
    "        pass \n"
   ],
   "id": "1ec8c64484e5d1b5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:59:33.136855Z",
     "start_time": "2025-05-21T07:49:08.384363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def measure_detailed_performance(env, episodes=10, max_cycles=25):\n",
    "    agent_ids = env.possible_agents\n",
    "    step_times = []\n",
    "    masking_times = []\n",
    "    gc_counts = []\n",
    "    fps_list = []\n",
    "    memory_snapshots = []\n",
    "    cpu_usages = []\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    tracemalloc.start()\n",
    "    gc_old = gc.get_count()\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        observations, _ = env.reset()\n",
    "        for _ in range(max_cycles):\n",
    "            actions = {\n",
    "                agent: env.action_space(agent).sample()\n",
    "                for agent in agent_ids if agent in observations\n",
    "            }\n",
    "\n",
    "            # CPU usage before step\n",
    "            cpu_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "            # Start timing full step\n",
    "            start_step = time.perf_counter()\n",
    "\n",
    "            # Execute step\n",
    "            observations, _, _, _, _ = env.step(actions)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            end_step = time.perf_counter()\n",
    "            cpu_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "            step_time = end_step - start_step\n",
    "            step_times.append(step_time)\n",
    "            fps_list.append(1.0 / step_time if step_time > 0 else 0)\n",
    "\n",
    "            # Record CPU\n",
    "            cpu_usages.append(cpu_after)\n",
    "\n",
    "            # Memory snapshot\n",
    "            memory_snapshots.append(process.memory_info().rss / (1024 ** 2))  # in MB\n",
    "\n",
    "            # GC delta\n",
    "            gc_counts.append(tuple(np.subtract(gc.get_count(), gc_old)))\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    return {\n",
    "        \"mean_step_time_ms\": np.mean(step_times) * 1000,\n",
    "        \"std_step_time_ms\": np.std(step_times) * 1000,\n",
    "        \"max_step_time_ms\": np.max(step_times) * 1000,\n",
    "        \"min_step_time_ms\": np.min(step_times) * 1000,\n",
    "        \"fps\": np.mean(fps_list),\n",
    "        \"gc_events\": np.sum(gc_counts, axis=0),\n",
    "        \"cpu_usage_mean\": np.mean(cpu_usages),\n",
    "        \"cpu_usage_std\": np.std(cpu_usages),\n",
    "        \"memory_usage_mean_MB\": np.mean(memory_snapshots),\n",
    "        \"memory_usage_std_MB\": np.std(memory_snapshots),\n",
    "        \"memory_peak_MB\": peak / (1024 ** 2)\n",
    "        \n",
    "    }\n",
    "def run_enhanced_overhead_test(agent_counts=[3, 10, 25], episodes=20, max_cycles=25):\n",
    "    results = []\n",
    "\n",
    "    for N in agent_counts:\n",
    "        print(f\"\\nüîç Evaluating {N} agents\")\n",
    "\n",
    "        # Baseline env\n",
    "        base_env = simple_spread_v3.env(N=N, max_cycles=max_cycles)\n",
    "        parallel_base = aec_to_parallel(base_env)\n",
    "        base_metrics = measure_detailed_performance(parallel_base, episodes, max_cycles)\n",
    "\n",
    "        # Failure_API env\n",
    "        env = simple_spread_v3.env(N=N, max_cycles=max_cycles)\n",
    "        agent_ids = env.possible_agents\n",
    "        failure_model = ProbabilisticModel(agent_ids=agent_ids, failure_prob=0.5)\n",
    "        wrapped = CommunicationWrapper(env, failure_models=[failure_model])\n",
    "        parallel_wrapped = aec_to_parallel(wrapped)\n",
    "        api_metrics = measure_detailed_performance(parallel_wrapped, episodes, max_cycles)\n",
    "\n",
    "        result = {\n",
    "            \"N\": N,\n",
    "            \"baseline_time_ms\": base_metrics[\"mean_step_time_ms\"],\n",
    "            \"api_time_ms\": api_metrics[\"mean_step_time_ms\"],\n",
    "            \"std_baseline_time\": base_metrics[\"std_step_time_ms\"],\n",
    "            \"std_api_time\": api_metrics[\"std_step_time_ms\"],\n",
    "            \"% overhead\": ((api_metrics[\"mean_step_time_ms\"] - base_metrics[\"mean_step_time_ms\"]) /\n",
    "                           base_metrics[\"mean_step_time_ms\"]) * 100,\n",
    "            \"baseline_fps\": base_metrics[\"fps\"],\n",
    "            \"api_fps\": api_metrics[\"fps\"],\n",
    "            \"baseline_cpu\": base_metrics[\"cpu_usage_mean\"],\n",
    "            \"api_cpu\": api_metrics[\"cpu_usage_mean\"],\n",
    "            \"baseline_mem_MB\": base_metrics[\"memory_usage_mean_MB\"],\n",
    "            \"api_mem_MB\": api_metrics[\"memory_usage_mean_MB\"],\n",
    "            \"baseline_mem_std\": base_metrics[\"memory_usage_std_MB\"],\n",
    "            \"api_mem_std\": api_metrics[\"memory_usage_std_MB\"],\n",
    "            \"baseline_gc\": base_metrics[\"gc_events\"],\n",
    "            \"api_gc\": api_metrics[\"gc_events\"],\n",
    "            \"api_memory_peak_MB\": api_metrics[\"memory_peak_MB\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n‚úÖ Test Complete. Summary:\")\n",
    "    print(df.to_string(index=False))\n",
    "    return df\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "def run_profiled_test():\n",
    "    run_enhanced_overhead_test(agent_counts=[3, 10, 25], episodes=20, max_cycles=25)\n",
    "\n",
    "# Create a profiler\n",
    "profiler = cProfile.Profile()\n",
    "\n",
    "# Run the function under profiler\n",
    "profiler.enable()\n",
    "run_profiled_test()\n",
    "profiler.disable()\n",
    "\n",
    "# Dump raw profile stats to a file (optional for Snakeviz or dot visualization)\n",
    "profiler.dump_stats(\"failure_api_profile.prof\")\n",
    "\n",
    "# Print top 30 cumulative time functions\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs().sort_stats(\"cumtime\").print_stats(30)\n"
   ],
   "id": "fa11d435492c5395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating 3 agents\n",
      "\n",
      "üîç Evaluating 10 agents\n",
      "\n",
      "üîç Evaluating 25 agents\n",
      "\n",
      "‚úÖ Test Complete. Summary:\n",
      " N  baseline_time_ms  api_time_ms  std_baseline_time  std_api_time  % overhead  baseline_fps  api_fps  baseline_cpu  api_cpu  baseline_mem_MB  api_mem_MB  baseline_mem_std  api_mem_std      baseline_gc         api_gc  api_memory_peak_MB\n",
      " 3        106.620634   114.496242           3.987345      8.282155    7.386570      9.391421 8.774578       27.3160  24.4180       207.085938  209.475281          0.230031     0.034146 [-34139, 163, 0] [159095, 0, 0]            0.174484\n",
      "10        137.364680   170.121686          18.110629     31.557654   23.846746      7.392553 6.017978       22.5568  24.6800       209.583211  209.605687          0.011582     0.014472    [27861, 0, 0]  [36174, 0, 0]            0.170699\n",
      "25        230.686591   471.760018          49.496126     30.188718  104.502575      4.460268 2.127794       24.8668  22.9356       209.844195  210.430109          0.116028     0.124369    [63819, 0, 0]  [81302, 0, 0]            0.474880\n",
      "         72386083 function calls (66033670 primitive calls) in 624.693 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 777 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000  624.701  312.350 interactiveshell.py:3634(run_code)\n",
      "      4/2    0.000    0.000  624.701  312.350 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  624.701  624.701 2648270807.py:1(<module>)\n",
      "        1    0.001    0.001  624.701  624.701 2648270807.py:114(run_profiled_test)\n",
      "        1    0.003    0.003  624.700  624.700 2648270807.py:65(run_enhanced_overhead_test)\n",
      "        6    0.516    0.086  623.915  103.986 2648270807.py:1(measure_detailed_performance)\n",
      "     3000    7.689    0.003  314.197    0.105 conversions.py:190(step)\n",
      "     3000  301.226    0.100  301.226    0.100 {built-in method time.sleep}\n",
      "    19000    0.600    0.000  148.397    0.008 communication_wrapper.py:231(last)\n",
      "   425520    1.664    0.000  119.426    0.000 order_enforcing.py:72(observe)\n",
      "851040/425520    1.846    0.000  117.762    0.000 base.py:40(observe)\n",
      "   425520   10.741    0.000  115.916    0.000 simple_env.py:139(observe)\n",
      "    19000    0.653    0.000  106.690    0.006 sharedobs_wrapper.py:43(last)\n",
      "   425596   96.123    0.000  103.945    0.000 simple_spread.py:189(observation)\n",
      "    57760    0.269    0.000  102.458    0.002 sharedobs_wrapper.py:26(observe)\n",
      "    19060    1.296    0.000  102.093    0.005 sharedobs_wrapper.py:35(observe_all)\n",
      "    39520    0.582    0.000  101.143    0.003 order_enforcing.py:62(step)\n",
      "79040/39520    0.282    0.000   99.778    0.003 base.py:46(step)\n",
      "    39520    0.858    0.000   99.619    0.003 assert_out_of_bounds.py:16(step)\n",
      "    39520    0.625    0.000   96.904    0.002 simple_env.py:244(step)\n",
      "     3000    0.645    0.000   94.198    0.031 simple_env.py:170(_execute_world_step)\n",
      "    19760    0.777    0.000   49.774    0.003 communication_wrapper.py:163(step)\n",
      "    19760    0.107    0.000   45.429    0.002 sharedobs_wrapper.py:20(step)\n",
      "  1816006    5.356    0.000   39.372    0.000 fromnumeric.py:2349(sum)\n",
      "     3000    0.138    0.000   38.850    0.013 core.py:119(step)\n",
      "     3000    9.756    0.003   37.139    0.012 core.py:149(apply_environment_force)\n",
      "4974231/2069122    9.985    0.000   34.075    0.000 base.py:21(__getattr__)\n",
      "  1816516   12.748    0.000   33.509    0.000 fromnumeric.py:69(_wrapreduction)\n",
      "  2025036    8.534    0.000   33.381    0.000 order_enforcing.py:39(__getattr__)\n",
      "    38820    0.366    0.000   30.782    0.001 communication_wrapper.py:68(_update_communication_state)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1b5a2786a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-21T07:59:33.137867Z"
    }
   },
   "cell_type": "code",
   "source": "df = run_enhanced_overhead_test()",
   "id": "2393d77e1df6a50c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating 3 agents\n",
      "\n",
      "üîç Evaluating 10 agents\n",
      "\n",
      "üîç Evaluating 25 agents\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "714ae228e90bb896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "411eee4b2589eb8e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
